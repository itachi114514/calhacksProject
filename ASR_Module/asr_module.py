import asyncio
import websockets
import numpy as np
import webrtcvad
import sounddevice as sd
import pickle as pkl
import torch
from torch import nn
from collections import deque, Counter
from dotenv import load_dotenv
import os
from tools.text_handle import clean_special_tags
load_dotenv()


class SpeechRecognizer:
    def __init__(self, server_uri,
                 model_type='speech'):
        self.server_uri = server_uri
        self.say = False
        self.model_type = model_type
        if model_type == 'Speech':
            from funasr import AutoModel
            model_path = "./src/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
            self.model = AutoModel(model=model_path, model_revision="v2.0.4",
                                   vad_model="fsmn-vad", vad_model_revision="v2.0.4",
                                   punc_model="ct-punc-c", punc_model_revision="v2.0.4",
                                   spk_model="cam++", spk_model_revision="v2.0.2",
                                   device="cuda")
            self.LANGUAGE = "zh"

        elif model_type == 'SenseVoice':
            from funasr import AutoModel
            model_path = './src/SenseVoiceSmall'
            self.model = AutoModel(model=model_path, model_revision="v2.0.4",
                                   vad_model="fsmn-vad", vad_model_revision="v2.0.4",
                                   punc_model="ct-punc-c", punc_model_revision="v2.0.4",
                                   # spk_model="cam++", spk_model_revision="v2.0.2",
                                   device="cuda")
            self.LANGUAGE = "zh"

        elif model_type == 'faster-whisper':
            from faster_whisper import WhisperModel
            self.model = WhisperModel("large-v3", device="cuda", compute_type="float16")
            self.LANGUAGE = "en"
            self.model.generate = lambda *args, **kwargs: self.model.transcribe(*args,  **kwargs)


        self.vad = webrtcvad.Vad(3)
        self.RATE = 16000
        self.CHUNK = int(self.RATE * (10 / 1000))
        self.MAX_QUEUE = 45  # è¯­è¨€è°ƒæ•´
        self.audio_buffer_deque = deque(maxlen=self.MAX_QUEUE)
        self.speech_history = deque(maxlen=self.MAX_QUEUE)


        self.MIN_TEXT_LENGTH = 3  # æœ€å°è½¬å½•æ–‡æœ¬é•¿åº¦
        self.DEVICE = 12
        self.stream = sd.InputStream(
            samplerate=self.RATE, channels=1, dtype=np.float32,
            blocksize=self.CHUNK,
            device=self.DEVICE
        )

        self.send_say_lock = asyncio.Lock()
        self.transcribe_audio_lock = asyncio.Lock()
        self.audio_buffer = np.array([])
        self.no_speech_counter = 0
        self.websocket = None
        self.cos_sim = nn.CosineSimilarity()
        self.noise_rms_queue=deque(maxlen=self.MAX_QUEUE)
        self.speak_rms_queue=deque(maxlen=self.MAX_QUEUE)


        self.NOISE_QUEUE_SIZE=self.MAX_QUEUE

        self.INITIAL_NOISE_THRESHOLD=0.20 # æœ€å°å£°éŸ³é˜ˆå€¼
        self.MAX_NOISE_THRESHOULD= float("inf") # æœ€å¤§çš„å£°éŸ³é˜ˆå€¼
        self.VALID_AUDIO_THRESHOLD = self.INITIAL_NOISE_THRESHOLD # èµ‹å€¼æœ€ä½æœ‰æ•ˆå£°éŸ³å¤§å°ä¸ºæœ€å°é˜ˆå€¼
        self.speak_rms_queue.append(self.INITIAL_NOISE_THRESHOLD) # æ·»åŠ ä¸€ä¸ªå€¼è¿›å»ä¿éšœå…¶ä¸€å®šä¸ä¸ºç©º
        with open("src/embedding.pkl", "rb") as f:
            self.embedding = pkl.load(f)

    async def connect_to_server(self):
        """è¿æ¥åˆ° WebSocket æœåŠ¡å™¨"""
        self.websocket = await websockets.connect(self.server_uri, ping_interval=20, ping_timeout=60, open_timeout=10)
        print(f"Connected to WebSocket Server: {self.server_uri}")

    async def send_text(self, text):
        """å‘é€è½¬å½•æ–‡æœ¬åˆ°æœåŠ¡å™¨"""
        if self.websocket:
            await self.websocket.send(f"DATA:{text}")


    async def send_text_if_long_enough(self, text):
        """å‘é€è½¬å½•æ–‡æœ¬åˆ°æœåŠ¡å™¨ï¼Œå¦‚æœé•¿åº¦è¶…è¿‡ä¸€å®šé˜ˆå€¼"""
        if self.model_type == "SenseVoice":
            # å»é™¤SenseVoiceè¿”å›çš„å¤šä½™è¾“å‡º
            text = clean_special_tags(text)
        print("è½¬å½•æ–‡æœ¬ï¼š{}".format(text))
        if len(text) > self.MIN_TEXT_LENGTH:
            await self.send_text(text=text)
        else:
            print(f"<send_text_if_long_enough>: Text too short, and discarded: {text}")

    async def send_say_state(self):
        if self.websocket:
            await self.websocket.send(f"SWITCH_STATE")

    async def switch_state(self):
        """åˆ‡æ¢çŠ¶æ€å¹¶å‘é€"""
        async with self.send_say_lock:  # ä½¿ç”¨é”æ¥ä¿è¯ä¸ä¼šåŒæ—¶è°ƒç”¨
            await self.send_say_state()
            self.say = not self.say

    def calculate_rms(self, audio_data):
        """è®¡ç®—éŸ³é¢‘æ•°æ®çš„å‡æ–¹æ ¹å€¼ (RMS)"""
        # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯æµ®ç‚¹ç±»å‹ï¼Œé¿å…æ•´æ•°æº¢å‡º
        try:
            audio_data = np.asarray(audio_data, dtype=np.float64)

            # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæ•°ç»„ï¼Œé¿å…é™¤ä»¥é›¶
            if audio_data.size == 0:
                return 0.0

            # è®¡ç®—RMS (å‡æ–¹æ ¹)
            return np.sqrt(np.mean(audio_data ** 2))
        except Exception as e:
            print(e)
            return 50.0

    def is_speech(self, audio_data):    # æœªæµ‹è¯•
        return np.sqrt(np.mean(audio_data ** 2)) > self.VALID_AUDIO_THRESHOLD
        """æ£€æµ‹æ˜¯å¦æ˜¯è¯­éŸ³ å¹¶è€ƒè™‘éŸ³é‡ï¼ˆè‡ªé€‚åº”é˜ˆå€¼ç‰ˆï¼‰"""
        speech_flag = self.vad.is_speech(audio_data.tobytes(), self.RATE)

        # è®¡ç®—å½“å‰éŸ³é¢‘ç‰‡æ®µçš„RMSå€¼
        rms = self.calculate_rms(audio_data)

        # å¦‚æœæ˜¯éè¯­éŸ³ç‰‡æ®µï¼Œæ›´æ–°å™ªéŸ³é˜ˆå€¼
        if not speech_flag:
            self.noise_rms_queue.append(rms)  # å°†RMSå€¼æ·»åŠ åˆ°é˜Ÿåˆ—
            # print(f"æ— æ´»åŠ¨éŸ³é¢‘çš„é˜ˆå€¼ {rms} {self.speech_history.count(0)}")
        else:
            self.speak_rms_queue.append(rms)
            print(f"æ´»åŠ¨éŸ³é¢‘çš„é˜ˆå€¼ {rms} {self.speech_history.count(0)}")

        # è®¡ç®—å½“å‰çš„å™ªéŸ³é˜ˆå€¼
        if len(self.noise_rms_queue) == self.NOISE_QUEUE_SIZE:
            # å½“é˜Ÿåˆ—å¡«æ»¡æ—¶ï¼Œè®¡ç®—é˜ˆå€¼ä¸ºå¹³å‡å€¼+2æ ‡å·®
            avg_noise_rms = np.mean(self.noise_rms_queue)
            std_noise_rms = np.std(self.noise_rms_queue)
            min_speak_rms=np.min(self.speak_rms_queue)

            self.VALID_AUDIO_THRESHOLD = min(self.MAX_NOISE_THRESHOULD,max(min_speak_rms+50,self.INITIAL_NOISE_THRESHOLD,avg_noise_rms + 2 * std_noise_rms))



        # æ£€æŸ¥æ˜¯å¦æ˜¯è¯­éŸ³
        if speech_flag: # åŒæ—¶æ»¡è¶³VADæ£€æµ‹å’ŒéŸ³é‡è¶…è¿‡åŠ¨æ€é˜ˆå€¼

            speech_flag = rms > self.VALID_AUDIO_THRESHOLD

        return speech_flag

    async def transcribe_audio(self, audio_data):
        """ä½¿ç”¨æ¨¡å‹è½¬å½•éŸ³é¢‘"""
        try:
            async with self.transcribe_audio_lock:  # ä½¿ç”¨é”æ¥ä¿è¯ä¸ä¼šåŒæ—¶è°ƒç”¨
                # sd.play(audio_data, samplerate=self.RATE, device=self.DEVICE)  # æ’­æ”¾éŸ³é¢‘
                kwargs = {}
                if self.model_type == "SenseVoice" or self.model_type == "Speech":
                    kwargs["output_timestamp"] = True
                print(0)
                audio_data = audio_data.astype(np.float32)  # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯float32ç±»å‹
                segments = self.model.generate(audio_data, beam_size=5,
                                       best_of=5,
                                       temperature=0.0,language=self.LANGUAGE, vad_filter=True, vad_parameters=dict(min_silence_duration_ms=500,speech_pad_ms=400),**kwargs)
                # print(list(segments[0]))
                if self.model_type == "faster-whisper":
                    segments = list(segments[0])  # faster-whisper è¿”å›çš„æ˜¯ç”Ÿæˆå™¨
                    segments = [{"text":segments[0].text}]
            if segments and segments[0]['text']:
                try:
                    if os.getenv("acoustic_fingerprint")!= "False":
                        dist = self.cos_sim(segments[0]["spk_embedding"],self.embedding)
                        if dist < 0.5:
                            print(f"{dist} lower than 0.5, ignored")
                            return
                    text = segments[0]['text']
                    #print(segments[0])
                    if self.model_type=="Speech":
                        speaker = segments[0]['sentence_info'][0]['spk']
                        print(f"Recognized: {text},Speaker: {speaker}")
                    # await self.send_text(f"TEXT:<{text}>CONF:<{dist}>")
                    await self.send_text_if_long_enough(f"{text}")
                    await asyncio.sleep(0.1)
                except:
                    print(segments)
        except Exception as e:
            print(e)
            pass

    async def test(self):
        """æµ‹è¯•å‡½æ•°"""
        # ç”¨äºä»¥æ–‡å­—è¾“å…¥ä»£æ›¿è¯­éŸ³å¹¶ç›´æ¥å‘é€åˆ°æœåŠ¡å™¨
        while True:
            text = input("Enter text to send: ")
            self.say = True
            await self.send_text(f"DATA:{text}")

    async def listen_and_transcribe(self, websocket):  # æœªæµ‹è¯•
        """ç›‘å¬éº¦å…‹é£å¹¶å‘é€æ•°æ®"""
        # await self.test()
        self.websocket = websocket
        print("ğŸ™ï¸ Listening...")
        self.stream.start()

        while True:
            audio_data = self.stream.read(self.CHUNK)[0]

            self.audio_buffer_deque.append(audio_data)
            # ä¸“é—¨ç”¨äºè®¡æ•°
            if self.is_speech(audio_data):
                self.speech_history.append(1)
            else:
                self.speech_history.append(0)

            # print(self.speech_history.count(1),self.speech_history.count(0))
            # å¦‚æœçŠ¶æ€æ˜¯è¯´è¯å°±ä¸€ç›´å¡«
            if self.say:
                # print(f'audio_data:{len(self.audio_buffer)}') # æ‰“å°é˜ˆå€¼
                self.audio_buffer = np.append(self.audio_buffer, audio_data)
                print(self.audio_buffer.size)
                print(self.speech_history.count(0) >= self.MAX_QUEUE * 0.8)
                print(self.say)
                print(self.speech_history)

            if self.say == False and self.speech_history.count(1) > self.MAX_QUEUE * 0.7:
                await self.switch_state()
                print('æ£€æµ‹åˆ°å¼€å§‹è¯´è¯')
                audio_buffer_data = list(self.audio_buffer_deque)
                for audio_data in audio_buffer_data:
                    self.audio_buffer = np.append(self.audio_buffer, audio_data)

                # self.audio_buffer_deque.clear()

            if self.say == True and self.speech_history.count(0) >= self.MAX_QUEUE * 0.8 and self.audio_buffer.size > 0:
                await self.switch_state()
                print('æ£€æµ‹åˆ°ç»“æŸè¯´è¯,å¼€å§‹è¯†åˆ«')
                print(f'æœ€ä½æœ‰æ•ˆå£°éŸ³é˜ˆå€¼ï¼š{self.VALID_AUDIO_THRESHOLD:.2f}')
                # print(len(self.audio_buffer))
                await self.transcribe_audio(self.audio_buffer)
                self.audio_buffer = np.array([])
                self.speech_history.clear()


            # self.audio_buffer_deque.clear()

    async def run(self):
        while True:
            try:
                async with websockets.connect(self.server_uri, ping_interval=20, ping_timeout=60,
                                              open_timeout=10) as websocket:
                    # await self.connect_to_server()

                    await self.listen_and_transcribe(websocket)
            except Exception as e:
                print(f'è¿œç¨‹è¿æ¥ä¸­æ–­ æŠ›å‡ºå¼‚å¸¸ï¼š{e} å°è¯•é‡æ–°é“¾æ¥')
                await asyncio.sleep(1)


if __name__ == "__main__":
    # å¦‚æœç±»å‹æ²¡æœ‰å°±é»˜è®¤ä¸ºSpeech
    model_type=os.getenv('ASR_MODEL_TYPE',"Speech")
    recognizer = SpeechRecognizer(server_uri="ws://localhost:8765?name=ASR_Module",model_type=model_type)
    asyncio.run(recognizer.run())