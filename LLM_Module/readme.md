# LLM 模块

该模块通过 WebSocket 与客户端进行对话交互，基于 OpenAI 的 GPT-4o-mini 实现角色扮演对话（当前人设为原神角色「胡桃」），可持续流式生成回答内容，并按句发送。支持中断、记忆聊天历史、根据外部传感器输入实现上下文关联。

---


## 🌟 功能说明

- 支持 GPT-4o-mini 的流式对话生成
- 支持角色扮演（胡桃）人格设定、风格和背景
- 句子级别的增量式输出，符合口语节奏
- 接收 `SAY:true` 指令中断当前生成内容
- 支持自定义世界设定输入（环境/人物/时间/事件）

---

## 🔧 使用方法

1. 修改配置（可选）：

   - 替换 `api_key` 为你的 OpenAI API 密钥  
   - 或切换为其他 API 兼容平台如 VolcEngine 等

2. 运行模块：

   ```bash
   python llm_stream.py
   ```

3. 模块将连接 WebSocket 地址：

   ```
   ws://localhost:8766?name=LLM_Module
   ```

   接收来自其它模块（如前端界面、麦克风监听模块）的 `QUESTION:` 指令，并返回分句输出的 `DATA:` 回答。

---

## ⚙️ 消息协议格式

| 消息类型            | 示例内容                                                                 |
|---------------------|--------------------------------------------------------------------------|
| `QUESTION:<text>`   | `QUESTION:你怎么看七七？` —— 发起对话请求                                 |
| `SAY:true`          | 中断当前回答生成                                                        |
| `DATA:<text>`       | 模块输出的分句内容，如：`DATA:当然啦，七七那家伙虽然不爱说话...`        |
| `END`               | 一次完整回答发送完毕后的结束信号                                       |

---

## 🧪 测试

你可以通过 WebSocket 客户端发送如下消息进行测试：

```text
QUESTION:你知道我是谁吗？
```

中断当前生成：

```text
SAY:true
```

